{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main working parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to scrape for the data. In this case, I scrape only for American restaurants in San Francisco, and later hope to get more data. \n",
    "\n",
    "Cory taught me to use the headers, otherwise sites seem to block me from scraping their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "url = 'http://sanfrancisco.menupages.com/restaurants/all-areas/all-neighborhoods/american/'\n",
    "page = requests.get(url,headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to scrape for restaurant menus. The first thing I need to do is obtain unique identifiers to append to the menu url to get the data for each restaurant.\n",
    "\n",
    "After some exploration, I found that the url for each restaurant can be found between the strings \"restaurants/\" and \"/\" in the html data. I then managed to use re to extract all the data, and store it in a variable called rests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rests = re.findall(r'restaurants/(.+?)/',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then found that there is an entry called advertise in there. I'm not sure how many there are, so I remove anything that looks like advertise and store it in a variable called amrests. I verified that I have 630 results for American restaurants on menupages, and exactly 630 entries in my list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"advertise\")\n",
    "amrests = [i for i in rests if not regex.search(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then want to obtain the restaurant names. I find that the names lie between the strings \" \"\\'> \" and \" < / a > \" (without the quotations and the spaces). I extract them and store them in a list called names. Once I remove all the empty strings, I verify that there are 630 names in this list as well.\n",
    "\n",
    "Sanity check later: Ensure all names are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = re.findall(r'\"\\'>(.*?)</a>',page.content)\n",
    "names = [i for i in names if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to pull the addresses of the restaurants. I found (somewhat) that these addresses lay between \" < br / > \" and the pipe symbol \" | \". To reference the pipe symbol in re, I had to use a backslash, which took a while to figure out and gives a hint for gotchas in the future.\n",
    "\n",
    "*I only got 619 addresses, so there is some inconsistency. For now, I think it's ok because for MVP I'm not looking to find the nearest restaurant, just a restaurant. Very important to come back to this later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adds = re.findall(r'<br/> (.+?) \\|',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the menu urls, I scrape the menupages websites and extract the menus. For american restaurants, there are 630 menus and this takes roughly 10 minutes to get through. I store the resuls in a hashtable (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_dict = {}\n",
    "t0 = time.time()\n",
    "\n",
    "for i in amrests:\n",
    "    url = 'http://sanfrancisco.menupages.com/restaurants/' + i + '/menu'\n",
    "    menu_dict[i] = requests.get(url,headers=headers).content\n",
    "t1 = time.time()\n",
    "totalt = t1-t0\n",
    "print totalt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, I ensure that there are 630 menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(menu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I store the messy menu html files in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.DataFrame(menu_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a view towards more categories, I categorize all of these items as American food. However, this will probably not be useful for MVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.loc[:,'Type'] = 'American'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to go home, so I pickled the dataframe, and also the page html content, restuarant urls, names and addresses. On Monday, I modified the dataframe, added names etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zip(amrests,names)\n",
    "#amrest_df = amrest_df.rename(columns={0:'URL',1:'HTML_Menu'})\n",
    "#amrest_df['Names'] = amrest_df['URL']\n",
    "#amrest_df.head()\n",
    "# name_url_dict = {}\n",
    "# for i,j in enumerate(amrests):\n",
    "#     name_url_dict[j] = names[i]\n",
    "#amrest_df['Names'] = amrest_df['Names'].map(name_url_dict)\n",
    "amrest_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.to_pickle('amrest_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obj0, obj1, obj2 are created here...\n",
    "# page_content = page.content\n",
    "# Saving the objects:\n",
    "# with open('objs.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([page_content, rests, names, adds], f)\n",
    "\n",
    "# Getting back the objects:\n",
    "with open('objs.pickle') as f:  # Python 3: open(..., 'rb')\n",
    "    page_content, rests, names, adds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Noise in present manner of data extraction (need to address later)*\n",
    "    \n",
    "1) Not taking into account the context (sometimes the price is listed as bacon for \\$9.49 but really it's meant to be a 2 egg breakfast for \\$9.49. This is pertinent in It's Top's Coffee Shop\n",
    "menu. Need a better way of handling this in the data\n",
    "    \n",
    "2) Some prices are in title. When price is non existent, need to pull from title (See It's Top's Coffee Shop House omlettes \\$7.95)\n",
    "\n",
    "3) Some items have weird pricing (See It's Top's Coffee Shop Side of eggs)\n",
    "\n",
    "4) Need to add context (Side of eggs vs breakfast consisting of eggs). This can be obtained from headers.\n",
    "\n",
    "5) Can extract more ingredients from description, but this will require manipulation of code to extract headers (see above) and text below headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting menu item, description and price from the garbled HTML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_df_dict ={}\n",
    "\n",
    "regexp_menu_item = r'<cite>(.+?)</cite>'\n",
    "el_menu_item = 0\n",
    "regexp_item_description = r'\\xa0(.+?)</th>'\n",
    "el_item_description = 0\n",
    "regexp_price = r'\\xa0(.+?)\\r'\n",
    "el_price = 1\n",
    "\n",
    "menu_index_start = 2\n",
    "\n",
    "for i in range(amrest_df.shape[0]):\n",
    "    menu_html = amrest_df['HTML_Menu'][i]\n",
    "    bs_menu = BeautifulSoup(menu_html,'html.parser')\n",
    "    menu_items_list = bs_menu.find_all('tr')\n",
    "    \n",
    "    menu_item_list = createItemList(menu_items_list[menu_index_start:],regexp_menu_item,el_menu_item)\n",
    "    item_description_list = createItemList(menu_items_list[menu_index_start:],regexp_item_description,el_item_description)\n",
    "    price_list = createItemList(menu_items_list[menu_index_start:],regexp_price,el_price)\n",
    "    \n",
    "    menu_df_dict[amrest_df['URL'][i]] = pd.DataFrame(zip(menu_item_list,item_description_list,price_list),columns=['Menu Item','Item Description','Price'])\n",
    "    \n",
    "def createItemList(bsoup_list,regexp,element):\n",
    "    new_list = []\n",
    "    for y in [re.findall(regexp,str(x)) for x in bsoup_list]:\n",
    "        try:\n",
    "            new_list.append(y[element])\n",
    "        except:\n",
    "            new_list.append(\"\")    \n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.read_pickle('amrest_df')\n",
    "\n",
    "# pickle.dump(menu_df_dict, open( \"menu_df_dict.p\", \"wb\" ) )\n",
    "\n",
    "menu_df_dict = pickle.load( open( \"menu_df_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Append the actual restaurant name, and the concatenated menu and item description strings to each menu dataframe that is associated with an individual restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,j in zip(amrest_df['URL'],amrest_df['Names']):\n",
    "    #menu_df_dict[i]['Menu Item + Description Text'] = menu_df_dict[i]['Menu Item'].map(str) + \" \" + menu_df_dict[i]['Item Description'].map(str)\n",
    "    menu_df_dict[i]['Restaurant Name'] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Concatenate the entire dictionary of dataframes together in one \"giant\" dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df = pd.concat(menu_df_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dateframe. For this week, start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_amrests_df.to_pickle('all_amrests_df')\n",
    "all_amrests_df = pd.read_pickle('all_amrests_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# Remove punctuations\n",
    "all_amrests_df['desc_list'] = all_amrests_df['Menu Item + Description Text'].apply(lambda x: x.translate(string.maketrans(\"\",\"\"), string.punctuation))\n",
    "# Tokenize\n",
    "all_amrests_df['desc_list'] = all_amrests_df['desc_list'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords, make everything lowercase\n",
    "all_amrests_df['desc_list'] = all_amrests_df['desc_list'].apply(lambda x: [i.lower() for i in x if i.lower() not in nltk.corpus.stopwords.words('english')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of list of tokens\n",
    "tokens_list = all_amrests_df.desc_list.tolist()\n",
    "\n",
    "# Get WordNet Lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens, so run, runs and running are all mapped to run\n",
    "all_amrests_df['desc_list_lem'] = [[lmtzr.lemmatize(i.decode('utf-8')) for i in x] for x in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the lemmatized tokens\n",
    "tokens = all_amrests_df['desc_list_lem'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the tokens using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False)\n",
    "tfs = tfidf.fit_transform(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfs\n",
    "with open('tfidfmat.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(tfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df['tfidf_vector'] = [i for i in tfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df.iloc[600].tfidf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the cosine similarity between an input documents and the rest of the documents, pull out the top 5 that look the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "# 500 works well\n",
    "# 300 also works very well\n",
    "#250\n",
    "# 40000 is interesting\n",
    "# 30000 works\n",
    "# 50 is an example of working poorly\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfs[2500], tfs).flatten()\n",
    "related_food_idcs = cosine_similarities.argsort()[::-1][1:6]\n",
    "\n",
    "#cosine_similarities[related_food_idcs]\n",
    "\n",
    "print related_food_idcs\n",
    "\n",
    "all_amrests_df.iloc[related_food_idcs][[\"Menu Item\",\"Item Description\",\"Restaurant Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table.iloc[18724]['Menu Item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_similarities.argsort()[::-1][1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df.iloc[299]['Item Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store results in a SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'rest_db'\n",
    "username = 'harisk87'\n",
    "pswd = '2PsWrD!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd_rest_table = pd_rest_table.rename(columns={'MenuItem': 'Menu Item', 'ItemDescription': 'Item Description','RestaurantName' : 'Restaurant Name'})\n",
    "#pd_rest_table = pd_rest_table.replace({'Restaurant Name': {'It&#39;s Top&#39;s Coffee Shop': 'It\\'s Tops Coffee Shop'}})\n",
    "pd_rest_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table['Restaurant Name'] = [x.decode('utf-8') for x in pd_rest_table['Restaurant Name']]\n",
    "pd_rest_table['Item Description'] = [x.decode('utf-8') for x in pd_rest_table['Item Description']]\n",
    "pd_rest_table['Menu Item'] = [x.decode('utf-8') for x in pd_rest_table['Menu Item']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_rest_table.to_pickle('pd_rest_table')\n",
    "pd_rest_table = pd.read_pickle('pd_rest_table')\n",
    "## pd_rest_table contains decoded version of stuff\n",
    "#pd_rest_table = pd.read_pickle('pd_rest_table')\n",
    "pd_rest_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_rest_table['tfidf_vector'].apply(lambda x: x.todense())\n",
    "#too big!\n",
    "pd_rest_table = pd_rest_table[[\"Menu Item\",\"Item Description\",\"Restaurant Name\",\"Price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table.to_sql('rest_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT * FROM rest_table;\n",
    "\"\"\"\n",
    "rest_data_from_sql = pd.read_sql_query(sql_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\" SELECT DISTINCT \"Restaurant Name\" FROM rest_table \"\"\"\n",
    "test = pd.read_sql_query(sql_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.iloc[1]['Restaurant Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rests = []\n",
    "#query_results = rest_data_from_sql\n",
    "query_results = pd_rest_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rests = []\n",
    "for i in range(0,query_results.shape[0]):\n",
    "    try:\n",
    "        rests.append(dict(index=query_results.iloc[i]['index'], menu_item=query_results.iloc[i]['Menu Item'], desc=query_results.iloc[i]['Item Description'], name = query_results.iloc[i]['Restaurant Name'], price = query_results.iloc[i]['Price']) )\n",
    "    except:\n",
    "        #rests.append(dict(index='Weird codec error', menu_item='Weird codec error', desc='Weird codec error', name = 'Weird codec error', price = 'Weird codec error') )\n",
    "        print i\n",
    "        rests = []\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(rests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_hack(text):\n",
    "    text_int = text.encode(\"utf-8\")\n",
    "    out_text = text_int.decode(\"utf-8\").encode('ascii','ignore')\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badidx = []\n",
    "for i in range(0,query_results.shape[0]):\n",
    "    index_print = query_results.iloc[i]['index']\n",
    "    \n",
    "    try:\n",
    "        menu_print = query_results.iloc[i]['Menu Item']\n",
    "        desc_print = query_results.iloc[i]['Item Description']\n",
    "        name_print = query_results.iloc[i]['Restaurant Name']\n",
    "        price_print = query_results.iloc[i]['Price']\n",
    "        \n",
    "        menu_print = encoding_hack(menu_print)\n",
    "        desc_print = encoding_hack(desc_print)\n",
    "        name_print = encoding_hack(name_print)\n",
    "        price_print = encoding_hack(price_print)\n",
    "        \n",
    "    except:\n",
    "        badidx.append(i)\n",
    "\n",
    "    \n",
    "\n",
    "    #rests.append(dict(index=index_print, menu_item=menu_print, desc=desc_print, name=name_print, price=price_print) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('badidx.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(badidx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('badidx.pickle') as f:  # Python 3: open(..., 'rb')\n",
    "    badidx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table_dropped = query_results[~query_results.index.isin(badidx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_rest_table_dropped.to_sql('rest_table', engine, if_exists='replace',index='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table_dropped.loc[903]['Menu Item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df[(all_amrests_df['Restaurant Name'] == 'Delancey Street') & (all_amrests_df['Menu Item'] == 'Filet Of Smoked Trout')].index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table[(pd_rest_table['Restaurant Name'] == 'Delancey Street') & (pd_rest_table['Menu Item'] == 'Filet Of Smoked Trout')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_rest_table.iloc[895]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PART 2: ATTEMPT TO GET CLEANER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "url = 'http://www.allmenus.com/ca/san-francisco/420269-thermidor/menu/'\n",
    "page = requests.get(url,headers=headers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(page.text, \"lxml\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://www.allmenus.com/ca/san-francisco/-/american/'\n",
    "page = requests.get(url,headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for urls\n",
      "URLs obtained, time to scrape for menus\n"
     ]
    }
   ],
   "source": [
    "food_categories = ['american','american-new','german','crepes','french','burgers','deli']#'asianfusion','californian','chinese','dim-sum','sandwiches'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "base_url = url = 'http://www.allmenus.com/ca/san-francisco/'\n",
    "\n",
    "rest_url_list = []\n",
    "category_list = []\n",
    "\n",
    "print 'Scraping for urls'\n",
    "for fcat in food_categories:\n",
    "    # Get all the restaurants that food category falls into\n",
    "    top_page = requests.get(base_url + '-/' + fcat + '/',headers=headers)\n",
    "    \n",
    "    # Turn it into a BeautifulSoup object\n",
    "    top_soup= BeautifulSoup(top_page.text, \"lxml\")\n",
    "    all_rest_links = top_soup.findAll(\"p\",{\"class\",\"restaurant_name\"})\n",
    "    \n",
    "    for rest in all_rest_links:\n",
    "        rest_url_list.append(rest.find('a')['href'])\n",
    "        category_list.append(fcat)\n",
    "    \n",
    "print 'URLs obtained, time to scrape for menus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076.99184704seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "item_list = []\n",
    "i=1\n",
    "for url,fcat in zip(rest_url_list,category_list):\n",
    "    #print \"Scraping restaurant %d\"%(i)\n",
    "    \n",
    "    page = requests.get('http://www.allmenus.com'+url,headers=headers)\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "    \n",
    "    # Extract restaurant name\n",
    "    name = unidecode(soup.find(\"h1\", {\"itemprop\":\"name\"}).text)\n",
    "\n",
    "    # Extract street address\n",
    "    saddr = unidecode(soup.find(\"span\",{\"itemprop\":\"streetAddress\"}).text)\n",
    "\n",
    "    # Extract the city\n",
    "    city = unidecode(soup.find(\"span\",{\"itemprop\":\"addressLocality\"}).text)\n",
    "\n",
    "    # Extract the state\n",
    "    state = unidecode(soup.find(\"span\",{\"itemprop\":\"addressRegion\"}).text)\n",
    "\n",
    "    # Extract the zip code\n",
    "    zipc = unidecode(soup.find(\"span\",{\"itemprop\":\"postalCode\"}).text)\n",
    "    \n",
    "    # Extract yelp rating\n",
    "    try:\n",
    "        yelp_rating = float(soup.find(\"meta\",{\"itemprop\":\"ratingValue\"})['content'])\n",
    "    except:\n",
    "        yelp_rating = None\n",
    "\n",
    "    # Extract number of yelp reviews\n",
    "    try:\n",
    "        num_yelp_reviews = int(soup.find(\"meta\",{\"itemprop\":\"reviewCount\"})['content'])\n",
    "    except:\n",
    "        num_yelp_reviews = None\n",
    "\n",
    "    # Get the yelp link\n",
    "    try:\n",
    "        yelp_link = soup.find(\"span\",{\"class\":\"review_count\"}).find('a')['href']\n",
    "    except:\n",
    "        yelp_link = None\n",
    "\n",
    "    all_categories = soup.find_all(\"div\",{\"class\":\"category\"})\n",
    "\n",
    "    for cat in all_categories:\n",
    "        category_name = unidecode(cat.find(\"div\",{\"class\":\"category_head\"}).h3.text)\n",
    "        category_description = unidecode(cat.find(\"div\",{\"class\":\"category_head\"}).p.text)\n",
    "    \n",
    "        all_menu_items_in_category = cat.find_all(\"li\",{\"class\":\"menu_item\"})\n",
    "    \n",
    "        for menu_item in all_menu_items_in_category:\n",
    "            item_name = unidecode(menu_item.find(\"span\",{\"class\":\"name\"}).text)\n",
    "            item_description = unidecode(menu_item.find(\"p\",{\"class\":\"description\"}).text)\n",
    "            try:\n",
    "                item_price = unidecode(menu_item.find(\"span\",{\"class\":\"price\"}).text)\n",
    "            except:\n",
    "                item_price = []\n",
    "        \n",
    "            new_item = {'restaurant_name':name, 'item_name':item_name,'item_description':item_description,'item_price':item_price,'category_name'\n",
    "                    :category_name,'category_description':category_description,'street_address':saddr,'city':city,\n",
    "                    'state':state,'zip':zipc,'full_address':\", \".join([saddr,city,state,zipc]),\"yelp_rating\":yelp_rating,\n",
    "                   'num_reviews':num_yelp_reviews,'yelp_link':yelp_link,'restaurant_category':fcat}\n",
    "            item_list.append(new_item)\n",
    "    i+=1\n",
    "\n",
    "test_df = pd.DataFrame(item_list)\n",
    "del item_list\n",
    "t1 = time.time()\n",
    "print str(t1-t0) + 'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_menus_rest_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "coercing to Unicode: need string or buffer, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-3b4e564126fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_menus_rest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all_menus_rest_df.p'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/harisk87/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \"\"\"\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_clipboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexcel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harisk87/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(obj, path)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mFile\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \"\"\"\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: coercing to Unicode: need string or buffer, list found"
     ]
    }
   ],
   "source": [
    "all_menus_rest_df.to_pickle('all_menus_rest_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_projects_env]",
   "language": "python",
   "name": "conda-env-my_projects_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
