{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main working parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to scrape for the data. In this case, I scrape only for American restaurants in San Francisco, and later hope to get more data. \n",
    "\n",
    "Cory taught me to use the headers, otherwise sites seem to block me from scraping their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "url = 'http://sanfrancisco.menupages.com/restaurants/all-areas/all-neighborhoods/american/'\n",
    "page = requests.get(url,headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to scrape for restaurant menus. The first thing I need to do is obtain unique identifiers to append to the menu url to get the data for each restaurant.\n",
    "\n",
    "After some exploration, I found that the url for each restaurant can be found between the strings \"restaurants/\" and \"/\" in the html data. I then managed to use re to extract all the data, and store it in a variable called rests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rests = re.findall(r'restaurants/(.+?)/',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then found that there is an entry called advertise in there. I'm not sure how many there are, so I remove anything that looks like advertise and store it in a variable called amrests. I verified that I have 630 results for American restaurants on menupages, and exactly 630 entries in my list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"advertise\")\n",
    "amrests = [i for i in rests if not regex.search(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then want to obtain the restaurant names. I find that the names lie between the strings \" \"\\'> \" and \" < / a > \" (without the quotations and the spaces). I extract them and store them in a list called names. Once I remove all the empty strings, I verify that there are 630 names in this list as well.\n",
    "\n",
    "Sanity check later: Ensure all names are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = re.findall(r'\"\\'>(.*?)</a>',page.content)\n",
    "names = [i for i in names if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to pull the addresses of the restaurants. I found (somewhat) that these addresses lay between \" < br / > \" and the pipe symbol \" | \". To reference the pipe symbol in re, I had to use a backslash, which took a while to figure out and gives a hint for gotchas in the future.\n",
    "\n",
    "*I only got 619 addresses, so there is some inconsistency. For now, I think it's ok because for MVP I'm not looking to find the nearest restaurant, just a restaurant. Very important to come back to this later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adds = re.findall(r'<br/> (.+?) \\|',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the menu urls, I scrape the menupages websites and extract the menus. For american restaurants, there are 630 menus and this takes roughly 10 minutes to get through. I store the resuls in a hashtable (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_dict = {}\n",
    "t0 = time.time()\n",
    "\n",
    "for i in amrests:\n",
    "    url = 'http://sanfrancisco.menupages.com/restaurants/' + i + '/menu'\n",
    "    menu_dict[i] = requests.get(url,headers=headers).content\n",
    "t1 = time.time()\n",
    "totalt = t1-t0\n",
    "print totalt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, I ensure that there are 630 menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(menu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I store the messy menu html files in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.DataFrame(menu_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a view towards more categories, I categorize all of these items as American food. However, this will probably not be useful for MVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.loc[:,'Type'] = 'American'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to go home, so I pickled the dataframe, and also the page html content, restuarant urls, names and addresses. On Monday, I modified the dataframe, added names etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zip(amrests,names)\n",
    "#amrest_df = amrest_df.rename(columns={0:'URL',1:'HTML_Menu'})\n",
    "#amrest_df['Names'] = amrest_df['URL']\n",
    "#amrest_df.head()\n",
    "# name_url_dict = {}\n",
    "# for i,j in enumerate(amrests):\n",
    "#     name_url_dict[j] = names[i]\n",
    "#amrest_df['Names'] = amrest_df['Names'].map(name_url_dict)\n",
    "amrest_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.to_pickle('amrest_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obj0, obj1, obj2 are created here...\n",
    "# page_content = page.content\n",
    "# Saving the objects:\n",
    "# with open('objs.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([page_content, rests, names, adds], f)\n",
    "\n",
    "# Getting back the objects:\n",
    "with open('objs.pickle') as f:  # Python 3: open(..., 'rb')\n",
    "    page_content, rests, names, adds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.read_pickle('amrest_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Noise in present manner of data extraction (need to address later)*\n",
    "    \n",
    "1) Not taking into account the context (sometimes the price is listed as bacon for \\$9.49 but really it's meant to be a 2 egg breakfast for \\$9.49. This is pertinent in It's Top's Coffee Shop\n",
    "menu. Need a better way of handling this in the data\n",
    "    \n",
    "2) Some prices are in title. When price is non existent, need to pull from title (See It's Top's Coffee Shop House omlettes \\$7.95)\n",
    "\n",
    "3) Some items have weird pricing (See It's Top's Coffee Shop Side of eggs)\n",
    "\n",
    "4) Need to add context (Side of eggs vs breakfast consisting of eggs). This can be obtained from headers.\n",
    "\n",
    "5) Can extract more ingredients from description, but this will require manipulation of code to extract headers (see above) and text below headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_df_dict ={}\n",
    "\n",
    "regexp_menu_item = r'<cite>(.+?)</cite>'\n",
    "el_menu_item = 0\n",
    "regexp_item_description = r'\\xa0(.+?)</th>'\n",
    "el_item_description = 0\n",
    "regexp_price = r'\\xa0(.+?)\\r'\n",
    "el_price = 1\n",
    "\n",
    "menu_index_start = 2\n",
    "\n",
    "for i in range(amrest_df.shape[0]):\n",
    "    menu_html = amrest_df['HTML_Menu'][i]\n",
    "    bs_menu = BeautifulSoup(menu_html,'html.parser')\n",
    "    menu_items_list = bs_menu.find_all('tr')\n",
    "    \n",
    "    menu_item_list = createItemList(menu_items_list[menu_index_start:],regexp_menu_item,el_menu_item)\n",
    "    item_description_list = createItemList(menu_items_list[menu_index_start:],regexp_item_description,el_item_description)\n",
    "    price_list = createItemList(menu_items_list[menu_index_start:],regexp_price,el_price)\n",
    "    \n",
    "    menu_df_dict[amrest_df['URL'][i]] = pd.DataFrame(zip(menu_item_list,item_description_list,price_list),columns=['Menu Item','Item Description','Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(menu_df_dict, open( \"menu_df_dict.p\", \"wb\" ) )\n",
    "\n",
    "menu_df_dict = pickle.load( open( \"menu_df_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_projects_env]",
   "language": "python",
   "name": "conda-env-my_projects_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
