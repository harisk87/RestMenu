{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main working parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to scrape for the data. In this case, I scrape only for American restaurants in San Francisco, and later hope to get more data. \n",
    "\n",
    "Cory taught me to use the headers, otherwise sites seem to block me from scraping their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "url = 'http://sanfrancisco.menupages.com/restaurants/all-areas/all-neighborhoods/american/'\n",
    "page = requests.get(url,headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to scrape for restaurant menus. The first thing I need to do is obtain unique identifiers to append to the menu url to get the data for each restaurant.\n",
    "\n",
    "After some exploration, I found that the url for each restaurant can be found between the strings \"restaurants/\" and \"/\" in the html data. I then managed to use re to extract all the data, and store it in a variable called rests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rests = re.findall(r'restaurants/(.+?)/',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then found that there is an entry called advertise in there. I'm not sure how many there are, so I remove anything that looks like advertise and store it in a variable called amrests. I verified that I have 630 results for American restaurants on menupages, and exactly 630 entries in my list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"advertise\")\n",
    "amrests = [i for i in rests if not regex.search(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then want to obtain the restaurant names. I find that the names lie between the strings \" \"\\'> \" and \" < / a > \" (without the quotations and the spaces). I extract them and store them in a list called names. Once I remove all the empty strings, I verify that there are 630 names in this list as well.\n",
    "\n",
    "Sanity check later: Ensure all names are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = re.findall(r'\"\\'>(.*?)</a>',page.content)\n",
    "names = [i for i in names if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to pull the addresses of the restaurants. I found (somewhat) that these addresses lay between \" < br / > \" and the pipe symbol \" | \". To reference the pipe symbol in re, I had to use a backslash, which took a while to figure out and gives a hint for gotchas in the future.\n",
    "\n",
    "*I only got 619 addresses, so there is some inconsistency. For now, I think it's ok because for MVP I'm not looking to find the nearest restaurant, just a restaurant. Very important to come back to this later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adds = re.findall(r'<br/> (.+?) \\|',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the menu urls, I scrape the menupages websites and extract the menus. For american restaurants, there are 630 menus and this takes roughly 10 minutes to get through. I store the resuls in a hashtable (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_dict = {}\n",
    "t0 = time.time()\n",
    "\n",
    "for i in amrests:\n",
    "    url = 'http://sanfrancisco.menupages.com/restaurants/' + i + '/menu'\n",
    "    menu_dict[i] = requests.get(url,headers=headers).content\n",
    "t1 = time.time()\n",
    "totalt = t1-t0\n",
    "print totalt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, I ensure that there are 630 menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(menu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I store the messy menu html files in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.DataFrame(menu_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a view towards more categories, I categorize all of these items as American food. However, this will probably not be useful for MVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.loc[:,'Type'] = 'American'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to go home, so I pickled the dataframe, and also the page html content, restuarant urls, names and addresses. On Monday, I modified the dataframe, added names etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zip(amrests,names)\n",
    "#amrest_df = amrest_df.rename(columns={0:'URL',1:'HTML_Menu'})\n",
    "#amrest_df['Names'] = amrest_df['URL']\n",
    "#amrest_df.head()\n",
    "# name_url_dict = {}\n",
    "# for i,j in enumerate(amrests):\n",
    "#     name_url_dict[j] = names[i]\n",
    "#amrest_df['Names'] = amrest_df['Names'].map(name_url_dict)\n",
    "amrest_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.to_pickle('amrest_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obj0, obj1, obj2 are created here...\n",
    "# page_content = page.content\n",
    "# Saving the objects:\n",
    "# with open('objs.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([page_content, rests, names, adds], f)\n",
    "\n",
    "# Getting back the objects:\n",
    "with open('objs.pickle') as f:  # Python 3: open(..., 'rb')\n",
    "    page_content, rests, names, adds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Noise in present manner of data extraction (need to address later)*\n",
    "    \n",
    "1) Not taking into account the context (sometimes the price is listed as bacon for \\$9.49 but really it's meant to be a 2 egg breakfast for \\$9.49. This is pertinent in It's Top's Coffee Shop\n",
    "menu. Need a better way of handling this in the data\n",
    "    \n",
    "2) Some prices are in title. When price is non existent, need to pull from title (See It's Top's Coffee Shop House omlettes \\$7.95)\n",
    "\n",
    "3) Some items have weird pricing (See It's Top's Coffee Shop Side of eggs)\n",
    "\n",
    "4) Need to add context (Side of eggs vs breakfast consisting of eggs). This can be obtained from headers.\n",
    "\n",
    "5) Can extract more ingredients from description, but this will require manipulation of code to extract headers (see above) and text below headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting menu item, description and price from the garbled HTML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_df_dict ={}\n",
    "\n",
    "regexp_menu_item = r'<cite>(.+?)</cite>'\n",
    "el_menu_item = 0\n",
    "regexp_item_description = r'\\xa0(.+?)</th>'\n",
    "el_item_description = 0\n",
    "regexp_price = r'\\xa0(.+?)\\r'\n",
    "el_price = 1\n",
    "\n",
    "menu_index_start = 2\n",
    "\n",
    "for i in range(amrest_df.shape[0]):\n",
    "    menu_html = amrest_df['HTML_Menu'][i]\n",
    "    bs_menu = BeautifulSoup(menu_html,'html.parser')\n",
    "    menu_items_list = bs_menu.find_all('tr')\n",
    "    \n",
    "    menu_item_list = createItemList(menu_items_list[menu_index_start:],regexp_menu_item,el_menu_item)\n",
    "    item_description_list = createItemList(menu_items_list[menu_index_start:],regexp_item_description,el_item_description)\n",
    "    price_list = createItemList(menu_items_list[menu_index_start:],regexp_price,el_price)\n",
    "    \n",
    "    menu_df_dict[amrest_df['URL'][i]] = pd.DataFrame(zip(menu_item_list,item_description_list,price_list),columns=['Menu Item','Item Description','Price'])\n",
    "    \n",
    "def createItemList(bsoup_list,regexp,element):\n",
    "    new_list = []\n",
    "    for y in [re.findall(regexp,str(x)) for x in bsoup_list]:\n",
    "        try:\n",
    "            new_list.append(y[element])\n",
    "        except:\n",
    "            new_list.append(\"\")    \n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.read_pickle('amrest_df')\n",
    "\n",
    "# pickle.dump(menu_df_dict, open( \"menu_df_dict.p\", \"wb\" ) )\n",
    "\n",
    "menu_df_dict = pickle.load( open( \"menu_df_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Append the actual restaurant name, and the concatenated menu and item description strings to each menu dataframe that is associated with an individual restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,j in zip(amrest_df['URL'],amrest_df['Names']):\n",
    "    #menu_df_dict[i]['Menu Item + Description Text'] = menu_df_dict[i]['Menu Item'].map(str) + \" \" + menu_df_dict[i]['Item Description'].map(str)\n",
    "    menu_df_dict[i]['Restaurant Name'] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Concatenate the entire dictionary of dataframes together in one \"giant\" dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df = pd.concat(menu_df_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dateframe. For this week, start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_amrests_df.to_pickle('all_amrests_df')\n",
    "all_amrests_df = pd.read_pickle('all_amrests_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# Remove punctuations\n",
    "all_amrests_df['desc_list'] = all_amrests_df['Menu Item + Description Text'].apply(lambda x: x.translate(string.maketrans(\"\",\"\"), string.punctuation))\n",
    "# Tokenize\n",
    "all_amrests_df['desc_list'] = all_amrests_df['desc_list'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harisk87/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/ipykernel/__main__.py:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords, make everything lowercase\n",
    "all_amrests_df['desc_list'] = all_amrests_df['desc_list'].apply(lambda x: [i.lower() for i in x if i.lower() not in nltk.corpus.stopwords.words('english')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of list of tokens\n",
    "tokens_list = all_amrests_df.desc_list.tolist()\n",
    "\n",
    "# Get WordNet Lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens, so run, runs and running are all mapped to run\n",
    "all_amrests_df['desc_list_lem'] = [[lmtzr.lemmatize(i.decode('utf-8')) for i in x] for x in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menu Item</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Menu Item + Description Text</th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>desc_list</th>\n",
       "      <th>desc_list_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two Egg Breakfasts</td>\n",
       "      <td>with home-fried potatoes &amp;amp; choice of rye, ...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>Two Egg Breakfasts with home-fried potatoes &amp;a...</td>\n",
       "      <td>It&amp;#39;s Top&amp;#39;s Coffee Shop</td>\n",
       "      <td>[two, egg, breakfasts, homefried, potatoes, am...</td>\n",
       "      <td>[two, egg, breakfast, homefried, potato, amp, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weekday Special</td>\n",
       "      <td>add one buttermilk hot cake to any egg breakfa...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>Weekday Special add one buttermilk hot cake to...</td>\n",
       "      <td>It&amp;#39;s Top&amp;#39;s Coffee Shop</td>\n",
       "      <td>[weekday, special, add, one, buttermilk, hot, ...</td>\n",
       "      <td>[weekday, special, add, one, buttermilk, hot, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Menu Item                                   Item Description  \\\n",
       "0  Two Egg Breakfasts  with home-fried potatoes &amp; choice of rye, ...   \n",
       "1     Weekday Special  add one buttermilk hot cake to any egg breakfa...   \n",
       "\n",
       "  Price                       Menu Item + Description Text  \\\n",
       "0  6.95  Two Egg Breakfasts with home-fried potatoes &a...   \n",
       "1  1.59  Weekday Special add one buttermilk hot cake to...   \n",
       "\n",
       "                  Restaurant Name  \\\n",
       "0  It&#39;s Top&#39;s Coffee Shop   \n",
       "1  It&#39;s Top&#39;s Coffee Shop   \n",
       "\n",
       "                                           desc_list  \\\n",
       "0  [two, egg, breakfasts, homefried, potatoes, am...   \n",
       "1  [weekday, special, add, one, buttermilk, hot, ...   \n",
       "\n",
       "                                       desc_list_lem  \n",
       "0  [two, egg, breakfast, homefried, potato, amp, ...  \n",
       "1  [weekday, special, add, one, buttermilk, hot, ...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amrests_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the lemmatized tokens\n",
    "tokens = all_amrests_df['desc_list_lem'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the tokens using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False)\n",
    "tfs = tfidf.fit_transform(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<51908x18518 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 375735 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the cosine similarity between an input documents and the rest of the documents, pull out the top 5 that look the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menu Item</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Restaurant Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sausage</td>\n",
       "      <td>in waffle</td>\n",
       "      <td>It&amp;#39;s Top&amp;#39;s Coffee Shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43212</th>\n",
       "      <td>Waffle</td>\n",
       "      <td></td>\n",
       "      <td>Allstar Donuts &amp;amp; Burgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39557</th>\n",
       "      <td>Waffle</td>\n",
       "      <td></td>\n",
       "      <td>Mission&amp;#39;s Kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46749</th>\n",
       "      <td>Waffle</td>\n",
       "      <td></td>\n",
       "      <td>Lakeside Cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44011</th>\n",
       "      <td>Waffle</td>\n",
       "      <td></td>\n",
       "      <td>Seal Rock Inn Restaurant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Menu Item Item Description                 Restaurant Name\n",
       "50      Sausage        in waffle  It&#39;s Top&#39;s Coffee Shop\n",
       "43212    Waffle                     Allstar Donuts &amp; Burgers\n",
       "39557    Waffle                            Mission&#39;s Kitchen\n",
       "46749    Waffle                                    Lakeside Cafe\n",
       "44011    Waffle                         Seal Rock Inn Restaurant"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "# 500 works well\n",
    "# 300 also works very well\n",
    "#250\n",
    "# 40000 is interesting\n",
    "# 30000 works\n",
    "# 50 is an example of working poorly\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfs[50], tfs).flatten()\n",
    "related_food_idcs = cosine_similarities.argsort()[:-6:-1]\n",
    "\n",
    "cosine_similarities[related_food_idcs]\n",
    "\n",
    "related_food_idcs\n",
    "\n",
    "all_amrests_df.iloc[related_food_idcs][[\"Menu Item\",\"Item Description\",\"Restaurant Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menu Item</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51903</th>\n",
       "      <td>Orange</td>\n",
       "      <td></td>\n",
       "      <td>The Kezar Pub</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51904</th>\n",
       "      <td>Cranberry</td>\n",
       "      <td></td>\n",
       "      <td>The Kezar Pub</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51905</th>\n",
       "      <td>Grapefruit</td>\n",
       "      <td></td>\n",
       "      <td>The Kezar Pub</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51906</th>\n",
       "      <td>Pineapple</td>\n",
       "      <td></td>\n",
       "      <td>The Kezar Pub</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51907</th>\n",
       "      <td>Tomato</td>\n",
       "      <td></td>\n",
       "      <td>The Kezar Pub</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Menu Item Item Description Restaurant Name Price\n",
       "51903      Orange                    The Kezar Pub      \n",
       "51904   Cranberry                    The Kezar Pub      \n",
       "51905  Grapefruit                    The Kezar Pub      \n",
       "51906   Pineapple                    The Kezar Pub      \n",
       "51907      Tomato                    The Kezar Pub      "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amrests_df[[\"Menu Item\",\"Item Description\",\"Restaurant Name\",\"Price\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to stem words before sticking it into the model. then can set min count\n",
    "\n",
    "# list comprehension, remove words not in model before predicting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TESTING STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding most similar items in a different category of food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty = fetch_20newsgroups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0329325   0.04457107  0.04433678  0.04507043  0.04865436]\n"
     ]
    }
   ],
   "source": [
    "print linear_kernel(tfidf[0:1], tfidf).flatten()[:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(sentences)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences,size=100,window=5,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'rest_db'\n",
    "username = 'harisk87'\n",
    "pswd = '2PsWrD!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://harisk87:2PsWrD!@localhost/rest_db\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_amrests_df[[\"Menu Item\",\"Item Description\",\"Restaurant Name\",\"Price\"]].to_sql('rest_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT * FROM rest_table WHERE index=600;\n",
    "\"\"\"\n",
    "rest_data_from_sql = pd.read_sql_query(sql_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Menu Item</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>Spinach Salad</td>\n",
       "      <td>add salmon</td>\n",
       "      <td>The Bell Tower</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      Menu Item Item Description Restaurant Name Price\n",
       "0    600  Spinach Salad       add salmon  The Bell Tower  6.00"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_data_from_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [my_projects_env]",
   "language": "python",
   "name": "Python [my_projects_env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
