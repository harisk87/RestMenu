{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main working parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to scrape for the data. In this case, I scrape only for American restaurants in San Francisco, and later hope to get more data. \n",
    "\n",
    "Cory taught me to use the headers, otherwise sites seem to block me from scraping their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "url = 'http://sanfrancisco.menupages.com/restaurants/all-areas/all-neighborhoods/american/'\n",
    "page = requests.get(url,headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to scrape for restaurant menus. The first thing I need to do is obtain unique identifiers to append to the menu url to get the data for each restaurant.\n",
    "\n",
    "After some exploration, I found that the url for each restaurant can be found between the strings \"restaurants/\" and \"/\" in the html data. I then managed to use re to extract all the data, and store it in a variable called rests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rests = re.findall(r'restaurants/(.+?)/',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then found that there is an entry called advertise in there. I'm not sure how many there are, so I remove anything that looks like advertise and store it in a variable called amrests. I verified that I have 630 results for American restaurants on menupages, and exactly 630 entries in my list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"advertise\")\n",
    "amrests = [i for i in rests if not regex.search(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then want to obtain the restaurant names. I find that the names lie between the strings \" \"\\'> \" and \" < / a > \" (without the quotations and the spaces). I extract them and store them in a list called names. Once I remove all the empty strings, I verify that there are 630 names in this list as well.\n",
    "\n",
    "Sanity check later: Ensure all names are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = re.findall(r'\"\\'>(.*?)</a>',page.content)\n",
    "names = [i for i in names if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to pull the addresses of the restaurants. I found (somewhat) that these addresses lay between \" < br / > \" and the pipe symbol \" | \". To reference the pipe symbol in re, I had to use a backslash, which took a while to figure out and gives a hint for gotchas in the future.\n",
    "\n",
    "*I only got 619 addresses, so there is some inconsistency. For now, I think it's ok because for MVP I'm not looking to find the nearest restaurant, just a restaurant. Very important to come back to this later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adds = re.findall(r'<br/> (.+?) \\|',page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the menu urls, I scrape the menupages websites and extract the menus. For american restaurants, there are 630 menus and this takes roughly 10 minutes to get through. I store the resuls in a hashtable (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_dict = {}\n",
    "t0 = time.time()\n",
    "\n",
    "for i in amrests:\n",
    "    url = 'http://sanfrancisco.menupages.com/restaurants/' + i + '/menu'\n",
    "    menu_dict[i] = requests.get(url,headers=headers).content\n",
    "t1 = time.time()\n",
    "totalt = t1-t0\n",
    "print totalt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, I ensure that there are 630 menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(menu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I store the messy menu html files in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.DataFrame(menu_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a view towards more categories, I categorize all of these items as American food. However, this will probably not be useful for MVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.loc[:,'Type'] = 'American'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to go home, so I pickled the dataframe, and also the page html content, restuarant urls, names and addresses. On Monday, I modified the dataframe, added names etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zip(amrests,names)\n",
    "#amrest_df = amrest_df.rename(columns={0:'URL',1:'HTML_Menu'})\n",
    "#amrest_df['Names'] = amrest_df['URL']\n",
    "#amrest_df.head()\n",
    "# name_url_dict = {}\n",
    "# for i,j in enumerate(amrests):\n",
    "#     name_url_dict[j] = names[i]\n",
    "#amrest_df['Names'] = amrest_df['Names'].map(name_url_dict)\n",
    "amrest_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df.to_pickle('amrest_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obj0, obj1, obj2 are created here...\n",
    "# page_content = page.content\n",
    "# Saving the objects:\n",
    "# with open('objs.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([page_content, rests, names, adds], f)\n",
    "\n",
    "# Getting back the objects:\n",
    "with open('objs.pickle') as f:  # Python 3: open(..., 'rb')\n",
    "    page_content, rests, names, adds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Noise in present manner of data extraction (need to address later)*\n",
    "    \n",
    "1) Not taking into account the context (sometimes the price is listed as bacon for \\$9.49 but really it's meant to be a 2 egg breakfast for \\$9.49. This is pertinent in It's Top's Coffee Shop\n",
    "menu. Need a better way of handling this in the data\n",
    "    \n",
    "2) Some prices are in title. When price is non existent, need to pull from title (See It's Top's Coffee Shop House omlettes \\$7.95)\n",
    "\n",
    "3) Some items have weird pricing (See It's Top's Coffee Shop Side of eggs)\n",
    "\n",
    "4) Need to add context (Side of eggs vs breakfast consisting of eggs). This can be obtained from headers.\n",
    "\n",
    "5) Can extract more ingredients from description, but this will require manipulation of code to extract headers (see above) and text below headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting menu item, description and price from the garbled HTML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "menu_df_dict ={}\n",
    "\n",
    "regexp_menu_item = r'<cite>(.+?)</cite>'\n",
    "el_menu_item = 0\n",
    "regexp_item_description = r'\\xa0(.+?)</th>'\n",
    "el_item_description = 0\n",
    "regexp_price = r'\\xa0(.+?)\\r'\n",
    "el_price = 1\n",
    "\n",
    "menu_index_start = 2\n",
    "\n",
    "for i in range(amrest_df.shape[0]):\n",
    "    menu_html = amrest_df['HTML_Menu'][i]\n",
    "    bs_menu = BeautifulSoup(menu_html,'html.parser')\n",
    "    menu_items_list = bs_menu.find_all('tr')\n",
    "    \n",
    "    menu_item_list = createItemList(menu_items_list[menu_index_start:],regexp_menu_item,el_menu_item)\n",
    "    item_description_list = createItemList(menu_items_list[menu_index_start:],regexp_item_description,el_item_description)\n",
    "    price_list = createItemList(menu_items_list[menu_index_start:],regexp_price,el_price)\n",
    "    \n",
    "    menu_df_dict[amrest_df['URL'][i]] = pd.DataFrame(zip(menu_item_list,item_description_list,price_list),columns=['Menu Item','Item Description','Price'])\n",
    "    \n",
    "def createItemList(bsoup_list,regexp,element):\n",
    "    new_list = []\n",
    "    for y in [re.findall(regexp,str(x)) for x in bsoup_list]:\n",
    "        try:\n",
    "            new_list.append(y[element])\n",
    "        except:\n",
    "            new_list.append(\"\")    \n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amrest_df = pd.read_pickle('amrest_df')\n",
    "\n",
    "# pickle.dump(menu_df_dict, open( \"menu_df_dict.p\", \"wb\" ) )\n",
    "\n",
    "menu_df_dict = pickle.load( open( \"menu_df_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Append the actual restaurant name, and the concatenated menu and item description strings to each menu dataframe that is associated with an individual restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,j in zip(amrest_df['URL'],amrest_df['Names']):\n",
    "    #menu_df_dict[i]['Menu Item + Description Text'] = menu_df_dict[i]['Menu Item'].map(str) + \" \" + menu_df_dict[i]['Item Description'].map(str)\n",
    "    menu_df_dict[i]['Restaurant Name'] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Concatenate the entire dictionary of dataframes together in one \"giant\" dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_amrests_df = pd.concat(menu_df_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dateframe. For this week, start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_amrests_df.to_pickle('all_amrests_df')\n",
    "all_amrests_df = pd.read_pickle('all_amrests_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# Remove punctuations\n",
    "all_amrests_df['desc_list'] = all_amrests_df['Menu Item + Description Text'].apply(lambda x: x.translate(string.maketrans(\"\",\"\"), string.punctuation))\n",
    "# Tokenize\n",
    "all_amrests_df['desc_list'] = all_amrests_df['desc_list'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harisk87/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/ipykernel/__main__.py:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords, make everything lowercase\n",
    "all_amrests_df['desc_list'] = all_amrests_df['desc_list'].apply(lambda x: [i.lower() for i in x if i.lower() not in nltk.corpus.stopwords.words('english')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of list of tokens\n",
    "tokens_list = all_amrests_df.desc_list.tolist()\n",
    "\n",
    "# Get WordNet Lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens, so run, runs and running are all mapped to run\n",
    "all_amrests_df['desc_list_lem'] = [[lmtzr.lemmatize(i.decode('utf-8')) for i in x] for x in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menu Item</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Menu Item + Description Text</th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>desc_list</th>\n",
       "      <th>desc_list_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two Egg Breakfasts</td>\n",
       "      <td>with home-fried potatoes &amp;amp; choice of rye, ...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>Two Egg Breakfasts with home-fried potatoes &amp;a...</td>\n",
       "      <td>It&amp;#39;s Top&amp;#39;s Coffee Shop</td>\n",
       "      <td>[two, egg, breakfasts, homefried, potatoes, am...</td>\n",
       "      <td>[two, egg, breakfast, homefried, potato, amp, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weekday Special</td>\n",
       "      <td>add one buttermilk hot cake to any egg breakfa...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>Weekday Special add one buttermilk hot cake to...</td>\n",
       "      <td>It&amp;#39;s Top&amp;#39;s Coffee Shop</td>\n",
       "      <td>[weekday, special, add, one, buttermilk, hot, ...</td>\n",
       "      <td>[weekday, special, add, one, buttermilk, hot, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Menu Item                                   Item Description  \\\n",
       "0  Two Egg Breakfasts  with home-fried potatoes &amp; choice of rye, ...   \n",
       "1     Weekday Special  add one buttermilk hot cake to any egg breakfa...   \n",
       "\n",
       "  Price                       Menu Item + Description Text  \\\n",
       "0  6.95  Two Egg Breakfasts with home-fried potatoes &a...   \n",
       "1  1.59  Weekday Special add one buttermilk hot cake to...   \n",
       "\n",
       "                  Restaurant Name  \\\n",
       "0  It&#39;s Top&#39;s Coffee Shop   \n",
       "1  It&#39;s Top&#39;s Coffee Shop   \n",
       "\n",
       "                                           desc_list  \\\n",
       "0  [two, egg, breakfasts, homefried, potatoes, am...   \n",
       "1  [weekday, special, add, one, buttermilk, hot, ...   \n",
       "\n",
       "                                       desc_list_lem  \n",
       "0  [two, egg, breakfast, homefried, potato, amp, ...  \n",
       "1  [weekday, special, add, one, buttermilk, hot, ...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amrests_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the lemmatized tokens\n",
    "tokens = all_amrests_df['desc_list_lem'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False)\n",
    "tfs = tfidf.fit_transform(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<51908x18518 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 375735 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menu Item</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Restaurant Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35888</th>\n",
       "      <td>French Press</td>\n",
       "      <td></td>\n",
       "      <td>Flour + Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17655</th>\n",
       "      <td>French Press</td>\n",
       "      <td></td>\n",
       "      <td>Moss Room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28785</th>\n",
       "      <td>French Press</td>\n",
       "      <td></td>\n",
       "      <td>Claudine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24312</th>\n",
       "      <td>French Press</td>\n",
       "      <td></td>\n",
       "      <td>SPQR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49517</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>french press</td>\n",
       "      <td>Manos Nouveau</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Menu Item Item Description Restaurant Name\n",
       "35888  French Press                    Flour + Water\n",
       "17655  French Press                        Moss Room\n",
       "28785  French Press                         Claudine\n",
       "24312  French Press                             SPQR\n",
       "49517        Coffee     french press   Manos Nouveau"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from scipy import sparse\n",
    "# 500 works well\n",
    "# 300 also works very well\n",
    "#250\n",
    "# 40000 is interesting\n",
    "# 30000 works\n",
    "\n",
    "cosine_similarities = linear_kernel(tfs[24312], tfs).flatten()\n",
    "related_food_idcs = cosine_similarities.argsort()[:-6:-1]\n",
    "\n",
    "cosine_similarities[related_food_idcs]\n",
    "\n",
    "related_food_idcs\n",
    "\n",
    "all_amrests_df.iloc[related_food_idcs][[\"Menu Item\",\"Item Description\",\"Restaurant Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to stem words before sticking it into the model. then can set min count\n",
    "\n",
    "# list comprehension, remove words not in model before predicting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TESTING STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding most similar items in a different category of food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty = fetch_20newsgroups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0329325   0.04457107  0.04433678  0.04507043  0.04865436]\n"
     ]
    }
   ],
   "source": [
    "print linear_kernel(tfidf[0:1], tfidf).flatten()[:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(sentences)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences,size=100,window=5,min_count=1,workers=4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [my_projects_env]",
   "language": "python",
   "name": "Python [my_projects_env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
